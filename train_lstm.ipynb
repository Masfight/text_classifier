{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa1e1e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060]\n",
      "[nltk_data]     Попытка установить соединение была безуспешной, т.к.\n",
      "[nltk_data]     от другого компьютера за требуемое время не получен\n",
      "[nltk_data]     нужный отклик, или было разорвано уже установленное\n",
      "[nltk_data]     соединение из-за неверного отклика уже подключенного\n",
      "[nltk_data]     компьютера>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion\n",
      "грусть       18430\n",
      "злость       17305\n",
      "радость      17099\n",
      "удивление    15260\n",
      "Name: count, dtype: int64\n",
      "Загружаем FastText эмбеддинги...\n",
      "Epoch 1: Loss 1.3855\n",
      "Epoch 2: Loss 0.5404\n",
      "Epoch 3: Loss 0.0152\n",
      "Epoch 4: Loss 0.0008\n",
      "Epoch 5: Loss 0.0004\n",
      "Epoch 6: Loss 0.0003\n",
      "Epoch 7: Loss 0.0002\n",
      "Epoch 8: Loss 0.0001\n",
      "Epoch 9: Loss 0.0001\n",
      "Epoch 10: Loss 0.0001\n",
      "Точность: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     радость       1.00      1.00      1.00      4275\n",
      "      грусть       1.00      1.00      1.00      4608\n",
      "      злость       1.00      1.00      1.00      4326\n",
      "   удивление       1.00      1.00      1.00      3815\n",
      "\n",
      "    accuracy                           1.00     17024\n",
      "   macro avg       1.00      1.00      1.00     17024\n",
      "weighted avg       1.00      1.00      1.00     17024\n",
      "\n",
      "Пример:  грусть\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# Скачиваем стоп-слова\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Загрузка БОЛЬШОГО датасета!\n",
    "df = pd.read_csv('data/ruemotext_sample1.csv', encoding='utf-8')\n",
    "df = df.dropna(subset=['text', 'emotion'])\n",
    "df = df[df['emotion'].isin(['радость', 'грусть', 'злость', 'удивление'])]\n",
    "print(df['emotion'].value_counts())\n",
    "\n",
    "df['tokens'] = df['text'].apply(preprocess)\n",
    "\n",
    "label2id = {e: i for i, e in enumerate(df['emotion'].unique())}\n",
    "id2label = {i: e for e, i in label2id.items()}\n",
    "df['label'] = df['emotion'].map(label2id)\n",
    "\n",
    "# Загрузка FastText-эмбеддингов (cc.ru.300.vec)\n",
    "def load_fasttext(path):\n",
    "    word2vec = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            parts = line.rstrip().split(' ')\n",
    "            word = parts[0]\n",
    "            vec = np.array(list(map(float, parts[1:])))\n",
    "            word2vec[word] = vec\n",
    "    return word2vec\n",
    "\n",
    "print(\"Загружаем FastText эмбеддинги...\")\n",
    "word2vec = load_fasttext('embeddings/cc.ru.300.vec')\n",
    "EMB_DIM = 300\n",
    "\n",
    "vocab = set([tok for toks in df['tokens'] for tok in toks])\n",
    "word2idx = {w: i+2 for i, w in enumerate(vocab)}\n",
    "word2idx['<PAD>'] = 0\n",
    "word2idx['<UNK>'] = 1\n",
    "\n",
    "def tokens2ids(tokens, max_len=20):\n",
    "    ids = [word2idx.get(tok, word2idx['<UNK>']) for tok in tokens]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [word2idx['<PAD>']] * (max_len - len(ids))\n",
    "    else:\n",
    "        ids = ids[:max_len]\n",
    "    return ids\n",
    "\n",
    "df['input_ids'] = df['tokens'].apply(tokens2ids)\n",
    "\n",
    "# Embedding-матрица\n",
    "embedding_matrix = np.zeros((len(word2idx), EMB_DIM))\n",
    "for w, idx in word2idx.items():\n",
    "    vec = word2vec.get(w)\n",
    "    if vec is not None:\n",
    "        embedding_matrix[idx] = vec\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(0, 1, EMB_DIM)\n",
    "\n",
    "X = np.stack(df['input_ids'].values)\n",
    "y = df['label'].values\n",
    "\n",
    "# Разделяем на train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "# Превращаем в тензоры\n",
    "X_train = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Создаём DataLoader'ы\n",
    "BATCH_SIZE = 256\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# LSTM-классификатор\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, n_classes, embedding_matrix):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.embedding.weight.data.copy_(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, n_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        h_n = h_n.squeeze(0)\n",
    "        out = self.fc(h_n)\n",
    "        return out\n",
    "\n",
    "model = LSTMClassifier(len(word2idx), EMB_DIM, 128, len(label2id), embedding_matrix)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Обучение с DataLoader\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X_batch)\n",
    "        loss = loss_fn(out, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}: Loss {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Оценка на тесте\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        out = model(X_batch)\n",
    "        preds = out.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "print('Точность:', accuracy_score(all_labels, all_preds))\n",
    "print(classification_report(all_labels, all_preds, target_names=[id2label[i] for i in range(len(id2label))]))\n",
    "\n",
    "# Пример: предсказание для нового текста\n",
    "def predict(text, max_len=20):\n",
    "    tokens = preprocess(text)\n",
    "    ids = tokens2ids(tokens, max_len)\n",
    "    tensor = torch.tensor([ids], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        pred = model(tensor)\n",
    "        label_id = pred.argmax(dim=1).item()\n",
    "        return id2label[label_id]\n",
    "\n",
    "print(\"Пример: \", predict(\"Это лучшая новость за сегодня!\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
